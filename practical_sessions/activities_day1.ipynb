{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "974f4ff1-4247-4b0f-af7b-d959af4b4023",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0420f-68b9-42ee-b163-fb3e14a8099f",
   "metadata": {},
   "source": [
    "The main goal of this notebook is to get used to the computational environment provided by [Colab](https://colab.research.google.com/). This environment provides free of charge access to computational resources suitable for learning and experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b8c37-1bba-4e75-9171-f16b86065031",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a547a-9494-4362-b843-22a9099e56a6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Before starting, we need to run the following two initialization cells. \n",
    "\n",
    "To run a cell, click on it and then either press Shift-Enter or click the \"play\" button on the toolbar at the top of the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daba05c-f054-4e1a-a62c-a1672b62607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab06a4-b92a-4826-a8d7-39a3e6b8a784",
   "metadata": {},
   "source": [
    "The cell above imports the module $\\mathtt{numpy}$, which contains highly efficient methods for numerical computation involving arrays.\n",
    "\n",
    "The next cell defines a class $\\mathtt{FiniteMDP}$, which is a simple implementation of a finite Markov Decision Process. It is not meant for \"production\" use, so there is not error checking or concerns with efficiency.\n",
    "\n",
    "There is a lot of code here, but don't be concerned with it.\n",
    "\n",
    "It is recommended that you collapse the **Initialization* section by clicking on the little triangle next to the section heading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd68144-527c-4b66-8738-3d948de361bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "class FiniteMDPWarning(UserWarning):\n",
    "    pass\n",
    "\n",
    "class FiniteMDP:\n",
    "    def __init__(self, states, actions, P, R, seed=None):\n",
    "        self.states = states.copy()\n",
    "        self.size = len(self.states)\n",
    "        self.state_index = {s: i for i, s in enumerate(states)}\n",
    "        self.actions = actions.copy()        \n",
    "        self.P = {\n",
    "            action:  np.array(M, dtype=np.float64, copy=True)\n",
    "            for action, M in P.items()\n",
    "        }\n",
    "\n",
    "        self.CP = {}\n",
    "        for action, P_a in self.P.items():        \n",
    "            self.CP[action] = np.cumsum(P_a, axis=1)\n",
    "            self.CP[action][:, -1] = 1.0\n",
    "        self.CP[action][:, -1] = 1.0\n",
    "\n",
    "        self.R = {\n",
    "            action:  np.array(M, dtype=np.float64, copy=True)\n",
    "            for action, M in R.items()\n",
    "        }\n",
    "        \n",
    "        self.P[None] = np.eye(self.size, dtype=np.float64)\n",
    "        self.R[None] = np.zeros(shape=(self.size, self.size), dtype=np.float64)\n",
    "        self.terminal_indices = {i for i, state in enumerate(states) if not actions[state]}\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.current_state_index = None\n",
    "        self.terminated = True        \n",
    "\n",
    "    def _policy_value_function_lu(self, indexed_policy, gamma):\n",
    "        PP = np.array([self.P[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        RR = np.array([self.R[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        b = np.sum(PP * RR, axis=1)\n",
    "        A = np.eye(self.size) - gamma * PP\n",
    "        for i in range(self.size):\n",
    "            if i in self.terminal_indices:\n",
    "                A[i][i] = 1.0\n",
    "                b[i] = 0\n",
    "        return np.linalg.solve(A, b)\n",
    "\n",
    "    def _policy_value_function_jacobi(self, indexed_policy, gamma, stop_tol, max_iterations, start=None):\n",
    "        if start is None:\n",
    "            V = np.zeros(self.size, dtype=np.float64)\n",
    "        else:\n",
    "            V = start.copy()\n",
    "        PP = np.array([self.P[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        RR = np.array([self.R[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        b = np.sum(PP * RR, axis=1)\n",
    "        for _ in range(max_iterations):\n",
    "            Vnew = b + gamma * PP @ V\n",
    "            sup_norm =  np.max(np.abs(V - Vnew))\n",
    "            if stop_tol is not None and sup_norm < stop_tol:\n",
    "                break\n",
    "            V = Vnew\n",
    "        else:\n",
    "            if stop_tol is not None:\n",
    "                warnings.warn(\n",
    "                    f'Maximum number of iterations reached in policy_value_function. Final delta: {sup_norm:5.3e}',\n",
    "                    FiniteMDPWarning,\n",
    "                    stacklevel=3,\n",
    "                )   \n",
    "        return V\n",
    "        \n",
    "    def _policy_value_function_gs(self, indexed_policy, gamma, stop_tol, max_iterations, start=None):\n",
    "        if start is None:\n",
    "            V = np.zeros(self.size, dtype=np.float64)\n",
    "        else:\n",
    "            V = start.copy()\n",
    "        PP = np.array([self.P[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        RR = np.array([self.R[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        b = np.sum(PP * RR, axis=1)\n",
    "        max_delta = -np.inf\n",
    "        for _ in range(max_iterations):\n",
    "            sup_norm = 0.0\n",
    "            for i in range(self.size):\n",
    "                new_value = b[i] + gamma * PP[i] @ V\n",
    "                sup_norm = max(sup_norm, abs(new_value - V[i]))\n",
    "                V[i] = new_value\n",
    "            if stop_tol is not None and sup_norm < stop_tol:\n",
    "                break\n",
    "        else:\n",
    "            if stop_tol is not None:\n",
    "                warnings.warn(\n",
    "                    f'Maximum number of iterations reached in policy_value_function. Final delta: {sup_norm:5.3e}',\n",
    "                    FiniteMDPWarning,\n",
    "                    stacklevel=3,\n",
    "                )   \n",
    "        return V\n",
    "        \n",
    "    def _policy_value_function(self, policy, gamma, method, stop_tol, max_iterations, start, return_type):\n",
    "        indexed_policy = self.size * [None]\n",
    "        for state, action in policy.items():\n",
    "            indexed_policy[self.state_index[state]] = action\n",
    "        match method:\n",
    "            case 'lu':\n",
    "                VV = self._policy_value_function_lu(indexed_policy, gamma)\n",
    "            case 'jacobi':\n",
    "                VV = self._policy_value_function_jacobi(indexed_policy, gamma, stop_tol, max_iterations, start)\n",
    "            case 'gs':\n",
    "                VV = self._policy_value_function_gs(indexed_policy, gamma, stop_tol, max_iterations, start)\n",
    "            case _:\n",
    "                raise ValueError(\"method should be 'lu', 'jacobi' or 'gs'\")\n",
    "\n",
    "        match return_type:\n",
    "            case 'dict':\n",
    "                return {state: VV[self.state_index[state]] for state in self.states}\n",
    "            case 'array':\n",
    "                return VV\n",
    "            case _:\n",
    "                raise ValueError(\"return_type must be 'dict' or 'array'\")\n",
    "\n",
    "    def policy_value_function(self, policy, gamma=1, method='lu', stop_tol=1E-8, max_iterations=100, start=None, return_type='dict'):\n",
    "        if start is not None:\n",
    "            start = np.array([start[s] for s in self.states])\n",
    "        return self._policy_value_function(policy, gamma, method, stop_tol, max_iterations, start, return_type)\n",
    "\n",
    "    def value_iteration(self, gamma=1, stop_tol=1E-8, max_iterations=100, start=None):\n",
    "        if start is None:\n",
    "            V = np.zeros(self.size, dtype=np.float64)\n",
    "        else:\n",
    "            V = np.array([start[s] for s in states], dtype=np.float64)\n",
    "            for i in range(self.size):\n",
    "                if i in self.terminal_indices:\n",
    "                    V[i] = 0.0\n",
    "        for _ in range(max_iterations):\n",
    "            max_delta = 0.0\n",
    "            for i, state in enumerate(self.states):\n",
    "                if i in self.terminal_indices:\n",
    "                    continue\n",
    "                new_value = -np.inf\n",
    "                for action in self.actions[state]:\n",
    "                    new_value = max(new_value, \n",
    "                                    sum(self.P[action][i, j] * (self.R[action][i, j] + gamma * V[j]) for j in range(self.size)))\n",
    "                if stop_tol is not None:\n",
    "                    max_delta = max(max_delta, abs(V[i] - new_value))\n",
    "                V[i] = new_value\n",
    "            if stop_tol is not None and max_delta < stop_tol:\n",
    "                break\n",
    "        else:\n",
    "                warnings.warn(\n",
    "                    f'Maximum number of iterations reached in policy_value_function. Final delta: {max_delta:5.3e}',\n",
    "                    FiniteMDPWarning,\n",
    "                    stacklevel=2,\n",
    "                )   \n",
    "        # Compute optimal policy\n",
    "        policy = {}\n",
    "        for i,state in enumerate(self.states):\n",
    "            if i in self.terminal_indices:\n",
    "                policy[state] = None\n",
    "                continue\n",
    "            max_value = -np.inf\n",
    "            max_action = None\n",
    "            for action in self.actions[state]:\n",
    "                new_value = sum(self.P[action][i, j] * (self.R[action][i, j] + gamma * V[j]) for j in range(self.size))\n",
    "                if new_value > max_value:\n",
    "                    max_value = new_value\n",
    "                    max_action = action\n",
    "            policy[state] = max_action\n",
    "\n",
    "        return {state: V[i] for i, state in enumerate(self.states)}, policy\n",
    "\n",
    "    def policy_iteration(self, gamma=1, method='lu', stop_tol=1E-8, max_iterations=100, relaxations=20, start=None):\n",
    "        if start is None:\n",
    "            V = np.zeros(self.size, dtype=np.float64)\n",
    "        else:\n",
    "            V = np.array([start[s] for s in states], dtype=np.float64)\n",
    "            for i in range(self.size):\n",
    "                if i in self.terminal_indices:\n",
    "                    V[i] = 0.0\n",
    "\n",
    "        # Initialize random policy\n",
    "        policy = {}\n",
    "        for i, state in enumerate(self.states):\n",
    "            if i in self.terminal_indices:\n",
    "                policy[state] = None\n",
    "                continue\n",
    "            policy[state] = self.rng.choice(self.actions[state])\n",
    "\n",
    "        # Compute approximate value of current policy\n",
    "        V = self._policy_value_function(policy, gamma, method, None, relaxations, None, 'array')\n",
    "\n",
    "        for _ in range(max_iterations):\n",
    "            # Compute improved policy\n",
    "            new_policy = {}\n",
    "            for i, state in enumerate(self.states):\n",
    "                if i in self.terminal_indices:\n",
    "                    new_policy[state] = None\n",
    "                    continue\n",
    "                max_value = -np.inf\n",
    "                max_action = None\n",
    "                for action in self.actions[state]:\n",
    "                    new_value = sum(self.P[action][i, j] * (self.R[action][i, j] + gamma * V[j]) for j in range(self.size))\n",
    "                    if new_value > max_value:\n",
    "                        max_value = new_value\n",
    "                        max_action = action\n",
    "                new_policy[state] = max_action\n",
    "\n",
    "            # Stop criterion 1: no change in optimal policy\n",
    "            if new_policy == policy:\n",
    "                 break\n",
    "            # Compute approximate value of improved policy\n",
    "            Vnew = self._policy_value_function(new_policy, gamma, method, None, relaxations, V, 'array')\n",
    "            # Stop criterion 2: Change in V smaller than stop_tol\n",
    "            if np.max(np.abs(Vnew - V)) < stop_tol:\n",
    "                break\n",
    "            # Update for next iteration\n",
    "            V = Vnew\n",
    "            policy = new_policy\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f'Maximum number of iterations reached in policy_value_function. Final delta: {max_delta:5.3e}',\n",
    "                FiniteMDPWarning,\n",
    "                stacklevel=2,\n",
    "            )   \n",
    "        # Compute higher precision approximation for value function of final policy\n",
    "        V = self._policy_value_function(policy, gamma, method, stop_tol, max_iterations, V, 'dict')\n",
    "\n",
    "        return V, policy\n",
    "\n",
    "    def reset(self, initial_state=None):\n",
    "        if initial_state is None:\n",
    "            while True:\n",
    "                index = self.rng.integers(0, len(self.states))\n",
    "                if index not in self.terminal_indices:\n",
    "                    break\n",
    "            self.current_state_index = index\n",
    "        else:\n",
    "            self.current_state_index = self.states.index(initial_state)\n",
    "        self.terminated = False\n",
    "        return self.states[self.current_state_index]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_state_index is None:\n",
    "            raise RuntimeError('MDP not initialized, call reset() before calling step() for the first time')\n",
    "        if self.terminated:\n",
    "            raise RuntimeError('run terminated, call reset() to start a new run')\n",
    "\n",
    "        u = self.rng.random()\n",
    "        next_state_index = np.searchsorted(self.CP[action][self.current_state_index], u, side='right')\n",
    "        next_state =  self.states[next_state_index]\n",
    "        self.terminated = next_state_index in self.terminal_indices\n",
    "        reward = self.R[action][self.current_state_index, next_state_index]\n",
    "        self.current_state_index = next_state_index\n",
    "        state = self.states[self.current_state_index]\n",
    "        \n",
    "        return (state, reward, self.terminated)\n",
    "        \n",
    "    def sarsa(\n",
    "        self,\n",
    "        gamma=1.0,\n",
    "        alpha=0.1,\n",
    "        epsilon=0.1,\n",
    "        n_episodes=100_000,\n",
    "        max_steps_per_episode=10_000,\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tabular SARSA(0) consistent with (P, R).\n",
    "\n",
    "        - Rewards on transitions into terminal states are allowed.\n",
    "        - Terminal states have no actions (actions[state] is None).\n",
    "        - No bootstrapping from terminal states.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Initialize Q(s,a) only for non-terminal states\n",
    "        # ------------------------------------------------------------\n",
    "        Q = {}\n",
    "        for s in self.states:\n",
    "            if self.actions[s] is None:\n",
    "                continue\n",
    "            for a in self.actions[s]:\n",
    "                Q[(s, a)] = 0.0\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Epsilon-greedy policy (greedy over Q)\n",
    "        # ------------------------------------------------------------\n",
    "        def epsilon_greedy_action(state):\n",
    "            actions = self.actions[state]\n",
    "            if actions is None:\n",
    "                return None\n",
    "\n",
    "            if rng.random() < epsilon:\n",
    "                return rng.choice(actions)\n",
    "\n",
    "            q_vals = [Q[(state, a)] for a in actions]\n",
    "            return actions[int(np.argmax(q_vals))]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # SARSA learning loop\n",
    "        # ------------------------------------------------------------\n",
    "        for _ in range(n_episodes):\n",
    "\n",
    "            state = self.reset()\n",
    "            state_index = self.state_index[state]\n",
    "\n",
    "            if state_index in self.terminal_indices:\n",
    "                continue\n",
    "\n",
    "            action = epsilon_greedy_action(state)\n",
    "\n",
    "            for _ in range(max_steps_per_episode):\n",
    "\n",
    "                i = self.state_index[state]\n",
    "\n",
    "                next_state, _, terminated = self.step(action)\n",
    "                j = self.state_index[next_state]\n",
    "\n",
    "                # IMPORTANT: reward always comes from R\n",
    "                reward = self.R[action][i, j]\n",
    "\n",
    "                if terminated:\n",
    "                    # no bootstrap from terminal states\n",
    "                    Q[(state, action)] += alpha * (\n",
    "                        reward - Q[(state, action)]\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                next_action = epsilon_greedy_action(next_state)\n",
    "\n",
    "                Q[(state, action)] += alpha * (\n",
    "                    reward\n",
    "                    + gamma * Q[(next_state, next_action)]\n",
    "                    - Q[(state, action)]\n",
    "                )\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Extract greedy policy and value function\n",
    "        # ------------------------------------------------------------\n",
    "        policy = {}\n",
    "        V = {}\n",
    "\n",
    "        for s in self.states:\n",
    "            idx = self.state_index[s]\n",
    "\n",
    "            if idx in self.terminal_indices:\n",
    "                policy[s] = None\n",
    "                V[s] = 0.0\n",
    "                continue\n",
    "\n",
    "            actions = self.actions[s]\n",
    "            q_vals = [(Q[(s, a)], a) for a in actions]\n",
    "            best_q, best_a = max(q_vals, key=lambda x: x[0])\n",
    "\n",
    "            policy[s] = best_a\n",
    "            V[s] = best_q\n",
    "\n",
    "        return Q, policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da8620-b00d-4b03-853f-72914043cc20",
   "metadata": {},
   "source": [
    "# Example - Recycling Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d21c56-3306-4ddc-8f64-fa5b09185e3e",
   "metadata": {},
   "source": [
    "In this first example, we use the Recycling Robot example from the book [*Reinforcement Learning - An Introduction* by Sutton and Barto](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)\n",
    "\n",
    "All you have to do is to run the cells and observe the results.\n",
    "\n",
    "The transition probability matrices for each action are:\n",
    "\n",
    "$$\n",
    "P^{\\mathtt{search}}=\n",
    "\\begin{bmatrix}\n",
    "\\alpha & 1-\\alpha\\\\\n",
    "1-\\beta & \\beta\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "P^{\\texttt{wait}}=\n",
    "\\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "P^\\mathtt{recharge}=\n",
    "\\begin{bmatrix}\n",
    "- & -\\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the reward matrices are:\n",
    "\n",
    "$$\n",
    "R^{\\mathtt{search}}=\n",
    "\\begin{bmatrix}\n",
    "r_{\\mathtt{search}} & r_{\\mathtt{search}}\\\\\n",
    "r_{\\mathtt{empty}} & r_{\\mathtt{search}}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "R^{\\mathtt{wait}}=\n",
    "\\begin{bmatrix}\n",
    "r_{\\mathtt{wait}} & - \\\\\n",
    "- & r_{\\mathtt{wait}}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "R^{\\mathtt{recharge}}=\n",
    "\\begin{bmatrix}\n",
    "- & -\\\\\n",
    "0 & -\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The dashes $-$ represent entries that are not meaningful, since they represent actions that are not allowed in a given state. They can have any value and, in practice, are symply set to zero. \n",
    "\n",
    "To represent the model computationally, we first define the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd73d8b5-7df6-4802-a71a-4899105a8be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "beta = 0.3\n",
    "rsearch = 15\n",
    "rwait = 10\n",
    "rempty = -3.0\n",
    "print(f'Model parameters:\\n{alpha=}, {beta=}, {rsearch=}, {rwait=}, {rempty=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15660b93-016b-4e42-bf6a-465c50e409ec",
   "metadata": {},
   "source": [
    "Let's now define the states and admissible actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e9a6d-53b0-43b9-a953-3e884e22c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['high', 'low']\n",
    "actions = {\n",
    "    'high': ['search', 'wait'],\n",
    "    'low': ['search', 'wait', 'recharge']\n",
    "}\n",
    "print(f\"Admissible actions for state 'high': {actions['high']}\\n\"\n",
    "      f\"Admissible actions for state 'low': {actions['low']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6b218-ee2a-4a2e-bd49-c9766892e832",
   "metadata": {},
   "source": [
    "Next, we define the transition probability and reward matrices for each action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1424a-c295-4a0b-830e-626c12a2131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = {}\n",
    "P['search'] = np.array([[alpha, 1 - alpha], [1 - beta, beta]], dtype=np.float64)\n",
    "P['wait'] = np.array([[1, 0], [0, 1]], dtype=np.float64)\n",
    "P['recharge'] = np.array([[0, 0], [1, 0]], dtype=np.float64)\n",
    "\n",
    "for key in P.keys():\n",
    "    print(f'Transition probability matrix for action {key}:')\n",
    "    print(P[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05bae7-a514-4ba6-9bd1-2a5112dd19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = {}\n",
    "R['search'] = np.array([[rsearch, rsearch], [rempty, rsearch]], dtype=np.float64)\n",
    "R['wait'] = np.array([[rwait, 0], [0, rwait]], dtype=np.float64)\n",
    "R['recharge'] = np.array([[0, 0], [0, 0]], dtype=np.float64)\n",
    "\n",
    "for key in R.keys():\n",
    "    print(f'Reward matrix for action {key}:')\n",
    "    print(R[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a7408-8228-4eae-858b-e10a2e1fd070",
   "metadata": {},
   "source": [
    "We are now ready to define the `FiniteMDP` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817191d-575a-47f0-9b19-184cfa11af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_model = FiniteMDP(states, actions, P, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79476cc-3335-44b2-a4f9-10be5d0db9ca",
   "metadata": {},
   "source": [
    "## Simulating the chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e5b09f-ee51-48a3-9fca-30de4158c7f0",
   "metadata": {},
   "source": [
    "Let's now simulate the chain. We start by defining a random number generator, to generate random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9812ac-60f5-4bd7-b05f-ff89d0d80748",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(77)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161cb084-23e0-47b1-ba82-239c098131c7",
   "metadata": {},
   "source": [
    "The next cell simulates $\\mathtt{n\\_steps}$ steps of the chain (recall that this is a continuing task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb7525-fdb9-4bd5-bb02-4f920f8e5026",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = rr_model.reset()\n",
    "n_steps = 20\n",
    "gamma = 0.9\n",
    "discount = 1.0\n",
    "total_return = 0.0\n",
    "print(f'Initial state: {current_state}')\n",
    "for i in range(n_steps):\n",
    "    action = rng.choice(actions[current_state])\n",
    "    next_state, reward, terminated = rr_model.step(action)\n",
    "    total_return += discount * reward\n",
    "    discount *= gamma\n",
    "    current_state = next_state\n",
    "    print(f'Step {i + 1:2d}: action={action:>9}, state={next_state:>5}, reward={reward:6.2f}, return={total_return:8.3f}, terminated={terminated}')\n",
    "    if terminated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439bab8-ebde-4575-a014-350d1cf0f72e",
   "metadata": {},
   "source": [
    "## Computation of State Value Function for a Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79468323-8f95-4fe8-86e3-3d97e0872274",
   "metadata": {},
   "source": [
    "We now turn to the problem of computing the state value function of a policy. In this case, the number of deterministic policies is small, so we just enumerate all policies in a list and compute the value function using different methods as a check.\n",
    "\n",
    "Computing the value function associated to a policy is called *evaluating* the policy and is a crucial step in any RL algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289c430-f15b-4381-a93e-5714922f96eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_policies = [\n",
    "    {'high': 'search', 'low': 'search'},\n",
    "    {'high': 'search', 'low': 'wait'},\n",
    "    {'high': 'search', 'low': 'recharge'},\n",
    "    {'high': 'wait', 'low': 'search'},\n",
    "    {'high': 'wait', 'low': 'wait'},\n",
    "    {'high': 'wait', 'low': 'recharge'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438d10c-109e-4740-bf27-df0c950756e4",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4162e9bf-d519-42aa-99a7-27e87bd58f41",
   "metadata": {},
   "source": [
    "\n",
    "In the next code cell, for each policy, we compute the returns for $\\mathtt{n\\_runs}$ of the Markov chain. The runs are truncated at $\\mathtt{n\\_steps}$ (we need to truncate because this is a continuing task).\n",
    "\n",
    "(A more efficient version of this code would use a \"first visit\" strategy in each run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca871b0-0a2d-43b9-bc69-9af4982b1e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "n_steps = 1000\n",
    "n_trials = 200\n",
    "\n",
    "for policy in rr_policies:\n",
    "    V = {'high': 0.0, 'low': 0.0}\n",
    "    for state in ['high', 'low']:\n",
    "        for n in range(n_trials):\n",
    "            discount = 1.0\n",
    "            current_state = rr_model.reset(initial_state=state)\n",
    "            total_return = 0.0\n",
    "            for i in range(n_steps):\n",
    "                action = policy[current_state]\n",
    "                next_state, reward, terminated = rr_model.step(action)\n",
    "                total_return += discount * reward\n",
    "                discount *= gamma\n",
    "                current_state = next_state\n",
    "                if terminated:\n",
    "                    break\n",
    "            V[state] += 1 / (n + 1) * (total_return - V[state])\n",
    "    print(f\"Policy: pi(high)={policy['high']}, pi(low)={policy['low']}. \"\n",
    "          f\"Value function: V(high)={V['high']:8.5f}, V(low)={V['low']:8.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b535b7-52b9-447b-bfed-b497f3e1a9fe",
   "metadata": {},
   "source": [
    "### LU Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75af64b2-340b-4b84-af0a-408d0b0a33eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rr_policy in rr_policies:\n",
    "    V = rr_model.policy_value_function(rr_policy, gamma=0.9)\n",
    "    print(f\"Policy: pi(high)={rr_policy['high']}, pi(low)={rr_policy['low']}. \"\n",
    "          f\"Value function: V(high)={V['high']:8.5f}, V(low)={V['low']:8.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0878a3-b274-4afd-bfef-31cfcd40f184",
   "metadata": {},
   "source": [
    "### Jacobi iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30053075-4dd3-4029-8188-465921890fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rr_policy in rr_policies:\n",
    "    V = rr_model.policy_value_function(rr_policy, gamma=0.9, method='jacobi', max_iterations=200)\n",
    "    print(f\"Policy: pi(high)={rr_policy['high']}, pi(low)={rr_policy['low']}. \"\n",
    "          f\"Value function: V(high)={V['high']:8.5f}, V(low)={V['low']:8.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f88fb-f7f8-49aa-80c8-f6cc7102224e",
   "metadata": {},
   "source": [
    "### Gauss-Seidel iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcd8256-6f8e-404a-94a3-af69ba5ca6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for rr_policy in rr_policies:\n",
    "    V = rr_model.policy_value_function(rr_policy, gamma=0.9, method='gs', max_iterations=200)\n",
    "    print(f\"Policy: pi(high)={rr_policy['high']}, pi(low)={rr_policy['low']}. \"\n",
    "          f\"Value function: V(high)={V['high']:8.5f}, V(low)={V['low']:8.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65783e0-fce6-49b1-a60a-b2f8c549d2db",
   "metadata": {},
   "source": [
    "# Defining a simple MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74db0c8-abe1-4216-9d1b-2bc36b59ebcc",
   "metadata": {},
   "source": [
    "Let's now define the MDP used in the Activities for day 1. Recall that the transition probabilities and rewards in this case are given by:\n",
    "\n",
    "$$\n",
    "P^a=\\begin{bmatrix}\n",
    "0.2 & 0.8\\\\\n",
    "0.7 & 0.3\n",
    "\\end{bmatrix}\\quad\n",
    "R^a=\\begin{bmatrix}\n",
    "10 & 7\\\\\n",
    "12 & 15\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P^b=\\begin{bmatrix}\n",
    "0.4 & 0.6\\\\\n",
    "0.1 & 0.9\n",
    "\\end{bmatrix}\\quad\n",
    "R^b=\\begin{bmatrix}\n",
    "5 & 11\\\\\n",
    "14 & 7\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P^c=\\begin{bmatrix}\n",
    "0.8 & 0.2\\\\\n",
    "0.2 & 0.8\n",
    "\\end{bmatrix}\\quad\n",
    "R^c=\\begin{bmatrix}\n",
    "14 & 3\\\\\n",
    "2 & 12\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91039d1-120c-4250-9e69-f6afed387fbe",
   "metadata": {},
   "source": [
    "The first thing we need to do is to define the states and actions admissible at each state. There are two states, $\\mathtt{1}$ and $\\mathtt{2}$, and three actions, $\\mathtt{'a'}$, $\\mathtt{'b'}$ and $\\mathtt{'c'}$. Complete the code in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e8563-6e80-49c9-a190-7bc75c90fe9d",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36662e34-1bdc-492e-aafd-7375accfab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the line below, replace the ... by the list of states\n",
    "states = [ ... ]\n",
    "# The line below shows how to define the actions for state 1. Do the same for state 2\n",
    "actions = {\n",
    "    1: ['a', 'b', 'c'],\n",
    "    2: [ ... ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19c3dbc-52ab-4a8b-8f01-c3875751ec41",
   "metadata": {},
   "source": [
    "We now need to define the transition probability matrices. The matrix for action $\\mathtt{'a'}$ is defined. Do the same for the other actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a1637-8a62-42c3-9f3c-5e627e9a6948",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = {}\n",
    "P['a'] = np.array([[0.2, 0.8], [0.7, 0.3]], dtype=np.float64)\n",
    "P['b'] = ...\n",
    "P['c'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4dbe61-d4cc-4285-899e-4039c80b1510",
   "metadata": {},
   "source": [
    "We can print the matrices to check that we have the right values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0faecd-094f-49b6-a348-5e7e4fbad975",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(P['a'])\n",
    "print(P['b'])\n",
    "print(P['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc0598-8722-48c5-a9a8-801b5ac66fe5",
   "metadata": {},
   "source": [
    "Now, in the cell below, define the reward matrices, following the pattern above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b73be-148a-4645-8ca1-b628a8c95b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = {}\n",
    "R['a'] = ...\n",
    "R['b'] = ...\n",
    "R['c'] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128158bb-db3a-4def-a323-fec475026986",
   "metadata": {},
   "source": [
    "Check that we have the correct values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4280c3f3-2b57-44cf-9e13-03d1873aac68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(R['a'])\n",
    "print(R['b'])\n",
    "print(R['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d975e-fbdd-4edb-a954-ef8ec88479ca",
   "metadata": {},
   "source": [
    "At this point, we have all the data needed to define the MDP. Run the following cell to create the object $\\mathtt{mdp\\_model}$ that represents our MDP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c41ce-6e1a-48f1-b59a-4898ae6dde9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_model = FiniteMDP(states, actions, P, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cab5ef3-7549-4942-8245-8a0cdaf98de3",
   "metadata": {},
   "source": [
    "Notice that there is no output. If there were no errors, then a Python object representing the MDP was created. If there were errors, check the definitions of your states, actions and transition and reward matrices. In the next sessions we will experiment with doing computations with the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961c552-029a-40fd-ba55-84c05ad35722",
   "metadata": {},
   "source": [
    "## Simulating a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d6f276-ca21-4135-9cdf-e927f215779e",
   "metadata": {},
   "source": [
    "One way to evaluate a policy is to simulate it. The cell below shows how to do this for the deterministic policy:\n",
    "$$\n",
    "\\pi(1)=\\mathtt{'c'},\\quad\\pi(2)=\\mathtt{'a'}\n",
    "$$\n",
    "When reading the simulation code, pay special attention to the following method calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4736e52-2b61-4e9d-b2b0-faddcef73280",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = {1: 'c', 2: 'a'}\n",
    "# Resets initial state\n",
    "current_state = mdp_model.reset(initial_state=1)\n",
    "n_steps = 10\n",
    "gamma = 0.9\n",
    "discount = 1.0\n",
    "total_return = 0.0\n",
    "print(f'Initial state: {current_state}')\n",
    "for i in range(n_steps):\n",
    "    # Select action according to current state\n",
    "    action = pi[current_state]\n",
    "    # Advances one step in the chain\n",
    "    next_state, reward, terminated = mdp_model.step(action)\n",
    "    # Add to total return and adjust discount\n",
    "    total_return += discount * reward\n",
    "    discount *= gamma\n",
    "    # Updates state for next iteration and print information about transition\n",
    "    current_state = next_state\n",
    "    print(f'Step {i + 1:2d}: action={action:>2}, state={next_state:>2}, reward={reward:6.2f}, '\n",
    "          f'return={total_return:8.3f}, terminated={terminated}')\n",
    "    if terminated:\n",
    "        break\n",
    "print(f'Total return for this run: {total_return}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d07bb5e-c681-467f-8e18-6db6fcb9ce49",
   "metadata": {},
   "source": [
    "Run the simulation several times. You will notice that the results are different each time. This is expected, since the simulation randomizes both the initial state and the transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b6b9bf-81f1-4275-9c77-5e6fc38e778a",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "This is a continuing task, so an interesting question is how close we are to the actual return for this policy. Experiment with the variable $\\mathtt{n\\_steps}$ to determine how many steps of the chain we have to simulate to get a reasonable estimate for the return.\n",
    "\n",
    "**Extra credit**: can you mathematically estimate the number of steps necessary to get an approximation with a given precision (say, $10^{-2}$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbdd274-a6d0-4af2-9d68-0eeddbeba53b",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Is the simulation as defined above sufficient to estimate what is the value function? If not, describe verbally what should be done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63131ad8-b15d-43d9-bad2-c8fbddc98040",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Repeat the simulation for initial state 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b941d-7459-44a5-9970-4ae5541237c5",
   "metadata": {},
   "source": [
    "## Computation of the value function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c5900-146d-4bc4-be2a-ad1db44a031c",
   "metadata": {},
   "source": [
    "Let's now see what are the methods available to evaluate a policy. We will consider the same policy as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b417d6-e7e3-42f7-b121-dee97f302eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = {1: 'c', 2: 'a'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9dc3c3-4b81-493c-9db4-19bbced1f9d7",
   "metadata": {},
   "source": [
    "## LU Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9618e26-adb0-4e5e-aeb0-5a4a72f98167",
   "metadata": {},
   "source": [
    "The first method we will demonstrate uses a LU decomposition. This is an exact linear algebra based in Gaussian elimination. It is equivalent to inverting the system matrix, but more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1a4d97-3f05-48cd-88a1-de6f67beba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = mdp_model.policy_value_function(pi, gamma=0.9)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98da792d-fa69-499f-8d6e-823e6906c17f",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "If we change the discount factor from $\\gamma=0.9$ to $\\gamma=0.8$ will the state value function increase or decrease? Confirm your answer by re-evaluating the value function with the new discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff03cd-8f34-41ea-8ac4-575ca657a495",
   "metadata": {},
   "source": [
    "## Jacobi iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946146ab-bd37-44d6-ad41-71858b92ee52",
   "metadata": {},
   "source": [
    "The next cell computes the value function using a Jacobi iteration. Recall that this method is not exact, and that the quality of the approximation depends on the number of iterations. (The method is guaranteed to converge if $\\gamma<1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a86ac0-0a4d-4dc1-96c9-1e64f6380e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = mdp_model.policy_value_function(pi, gamma=0.9, method='jacobi', max_iterations=200)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81666e3-5475-44d2-914d-cded4b493067",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "What happens if the parameter $\\mathtt{max\\_iterations}$ is too small? Try to find what is the minimum number of iteration that produces an acceptable result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f7f49e-1817-4f01-9a33-5a41c3fccdd9",
   "metadata": {},
   "source": [
    "## Gauss-Seidel iteration\n",
    "\n",
    "The next cell shows how to run a Gauss-Seidel iteration. This method tends to be slightly more efficient than a Jacobi iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc8921f-e2d0-4878-b127-0ecaa461eb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = mdp_model.policy_value_function(pi, gamma=0.9, method='gs', max_iterations=200)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f344dd-bf86-4389-898f-41a8fd0af0df",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Try to find the minimum number of iterations that produce a \"good\" result both with Jacobi and Gauss-Seidel. Is it possible to determine, in this case, which method finds the answer in fewer interations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af84e956-504c-4b5d-bd41-98ff6ecf69c5",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Repeat the experiments above with a different policy. If you are feeling brave, try to find the optimal policy by computing the value function of all policies (there are only 8 deteministic policies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8cf1c1-19aa-4e59-b278-b86c70ea4795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
