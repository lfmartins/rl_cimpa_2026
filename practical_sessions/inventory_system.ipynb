{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964f0ca8-5600-4032-903a-c89f63398f9d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb66d5ae-5a4f-4ae5-b517-384ce5d4700a",
   "metadata": {},
   "source": [
    "Run the following cell to initialize the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3de51f7a-1b4b-44ec-bd83-28190347ee25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "class FiniteMDPWarning(UserWarning):\n",
    "    pass\n",
    "\n",
    "class FiniteMDP:\n",
    "    def __init__(self, states, actions, P, R, seed=None):\n",
    "        self.states = states.copy()\n",
    "        self.size = len(self.states)\n",
    "        self.state_index = {s: i for i, s in enumerate(states)}\n",
    "        self.actions = actions.copy()        \n",
    "        self.P = {\n",
    "            action:  np.array(M, dtype=np.float64, copy=True)\n",
    "            for action, M in P.items()\n",
    "        }\n",
    "\n",
    "        self.CP = {}\n",
    "        for action, P_a in self.P.items():        \n",
    "            self.CP[action] = np.cumsum(P_a, axis=1)\n",
    "            self.CP[action][:, -1] = 1.0\n",
    "        self.CP[action][:, -1] = 1.0\n",
    "\n",
    "        self.R = {\n",
    "            action:  np.array(M, dtype=np.float64, copy=True)\n",
    "            for action, M in R.items()\n",
    "        }\n",
    "        \n",
    "        self.P[None] = np.eye(self.size, dtype=np.float64)\n",
    "        self.R[None] = np.zeros(shape=(self.size, self.size), dtype=np.float64)\n",
    "        self.terminal_indices = {i for i, state in enumerate(states) if not actions[state]}\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.current_state_index = None\n",
    "        self.terminated = True        \n",
    "\n",
    "    def _policy_value_function_lu(self, indexed_policy, gamma):\n",
    "        PP = np.array([self.P[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        RR = np.array([self.R[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        b = np.sum(PP * RR, axis=1)\n",
    "        A = np.eye(self.size) - gamma * PP\n",
    "        for i in range(self.size):\n",
    "            if i in self.terminal_indices:\n",
    "                A[i][i] = 1.0\n",
    "                b[i] = 0\n",
    "        return np.linalg.solve(A, b)\n",
    "\n",
    "    def _policy_value_function_jacobi(self, indexed_policy, gamma, stop_tol, max_iterations, start=None):\n",
    "        if start is None:\n",
    "            V = np.zeros(self.size, dtype=np.float64)\n",
    "        else:\n",
    "            V = start.copy()\n",
    "        PP = np.array([self.P[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        RR = np.array([self.R[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        b = np.sum(PP * RR, axis=1)\n",
    "        for _ in range(max_iterations):\n",
    "            Vnew = b + gamma * PP @ V\n",
    "            sup_norm =  np.max(np.abs(V - Vnew))\n",
    "            if stop_tol is not None and sup_norm < stop_tol:\n",
    "                break\n",
    "            V = Vnew\n",
    "        else:\n",
    "            if stop_tol is not None:\n",
    "                warnings.warn(\n",
    "                    f'Maximum number of iterations reached in policy_value_function. Final delta: {sup_norm:5.3e}',\n",
    "                    FiniteMDPWarning,\n",
    "                    stacklevel=3,\n",
    "                )   \n",
    "        return V\n",
    "        \n",
    "    def _policy_value_function_gs(self, indexed_policy, gamma, stop_tol, max_iterations, start=None):\n",
    "        if start is None:\n",
    "            V = np.zeros(self.size, dtype=np.float64)\n",
    "        else:\n",
    "            V = start.copy()\n",
    "        PP = np.array([self.P[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        RR = np.array([self.R[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        b = np.sum(PP * RR, axis=1)\n",
    "        max_delta = -np.inf\n",
    "        for _ in range(max_iterations):\n",
    "            sup_norm = 0.0\n",
    "            for i in range(self.size):\n",
    "                new_value = b[i] + gamma * PP[i] @ V\n",
    "                sup_norm = max(sup_norm, abs(new_value - V[i]))\n",
    "                V[i] = new_value\n",
    "            if stop_tol is not None and sup_norm < stop_tol:\n",
    "                break\n",
    "        else:\n",
    "            if stop_tol is not None:\n",
    "                warnings.warn(\n",
    "                    f'Maximum number of iterations reached in policy_value_function. Final delta: {sup_norm:5.3e}',\n",
    "                    FiniteMDPWarning,\n",
    "                    stacklevel=3,\n",
    "                )   \n",
    "        return V\n",
    "        \n",
    "    def _policy_value_function(self, policy, gamma, method, stop_tol, max_iterations, start, return_type):\n",
    "        indexed_policy = self.size * [None]\n",
    "        for state, action in policy.items():\n",
    "            indexed_policy[self.state_index[state]] = action\n",
    "        match method:\n",
    "            case 'lu':\n",
    "                VV = self._policy_value_function_lu(indexed_policy, gamma)\n",
    "            case 'jacobi':\n",
    "                VV = self._policy_value_function_jacobi(indexed_policy, gamma, stop_tol, max_iterations, start)\n",
    "            case 'gs':\n",
    "                VV = self._policy_value_function_gs(indexed_policy, gamma, stop_tol, max_iterations, start)\n",
    "            case _:\n",
    "                raise ValueError(\"method should be 'lu', 'jacobi' or 'gs'\")\n",
    "\n",
    "        match return_type:\n",
    "            case 'dict':\n",
    "                return {state: VV[self.state_index[state]] for state in self.states}\n",
    "            case 'array':\n",
    "                return VV\n",
    "            case _:\n",
    "                raise ValueError(\"return_type must be 'dict' or 'array'\")\n",
    "\n",
    "    def policy_value_function(self, policy, gamma=1, method='lu', stop_tol=1E-8, max_iterations=100, start=None, return_type='dict'):\n",
    "        if start is not None:\n",
    "            start = np.array([start[s] for s in self.states])\n",
    "        return self._policy_value_function(policy, gamma, method, stop_tol, max_iterations, start, return_type)\n",
    "\n",
    "    def value_iteration(self, gamma=1, stop_tol=1E-8, max_iterations=100, start=None):\n",
    "        if start is None:\n",
    "            V = np.zeros(self.size, dtype=np.float64)\n",
    "        else:\n",
    "            V = np.array([start[s] for s in states], dtype=np.float64)\n",
    "            for i in range(self.size):\n",
    "                if i in self.terminal_indices:\n",
    "                    V[i] = 0.0\n",
    "        for _ in range(max_iterations):\n",
    "            max_delta = 0.0\n",
    "            for i, state in enumerate(self.states):\n",
    "                if i in self.terminal_indices:\n",
    "                    continue\n",
    "                new_value = -np.inf\n",
    "                for action in self.actions[state]:\n",
    "                    new_value = max(new_value, \n",
    "                                    sum(self.P[action][i, j] * (self.R[action][i, j] + gamma * V[j]) for j in range(self.size)))\n",
    "                if stop_tol is not None:\n",
    "                    max_delta = max(max_delta, abs(V[i] - new_value))\n",
    "                V[i] = new_value\n",
    "            if stop_tol is not None and max_delta < stop_tol:\n",
    "                break\n",
    "        else:\n",
    "                warnings.warn(\n",
    "                    f'Maximum number of iterations reached in policy_value_function. Final delta: {max_delta:5.3e}',\n",
    "                    FiniteMDPWarning,\n",
    "                    stacklevel=2,\n",
    "                )   \n",
    "        # Compute optimal policy\n",
    "        policy = {}\n",
    "        for i,state in enumerate(self.states):\n",
    "            if i in self.terminal_indices:\n",
    "                policy[state] = None\n",
    "                continue\n",
    "            max_value = -np.inf\n",
    "            max_action = None\n",
    "            for action in self.actions[state]:\n",
    "                new_value = sum(self.P[action][i, j] * (self.R[action][i, j] + gamma * V[j]) for j in range(self.size))\n",
    "                if new_value > max_value:\n",
    "                    max_value = new_value\n",
    "                    max_action = action\n",
    "            policy[state] = max_action\n",
    "\n",
    "        return {state: V[i] for i, state in enumerate(self.states)}, policy\n",
    "\n",
    "    def policy_iteration(self, gamma=1, method='lu', stop_tol=1E-8, max_iterations=100, relaxations=20, start=None):\n",
    "        if start is None:\n",
    "            V = np.zeros(self.size, dtype=np.float64)\n",
    "        else:\n",
    "            V = np.array([start[s] for s in states], dtype=np.float64)\n",
    "            for i in range(self.size):\n",
    "                if i in self.terminal_indices:\n",
    "                    V[i] = 0.0\n",
    "\n",
    "        # Initialize random policy\n",
    "        policy = {}\n",
    "        for i, state in enumerate(self.states):\n",
    "            if i in self.terminal_indices:\n",
    "                policy[state] = None\n",
    "                continue\n",
    "            policy[state] = self.rng.choice(self.actions[state])\n",
    "\n",
    "        # Compute approximate value of current policy\n",
    "        V = self._policy_value_function(policy, gamma, method, None, relaxations, None, 'array')\n",
    "\n",
    "        for _ in range(max_iterations):\n",
    "            # Compute improved policy\n",
    "            new_policy = {}\n",
    "            for i, state in enumerate(self.states):\n",
    "                if i in self.terminal_indices:\n",
    "                    new_policy[state] = None\n",
    "                    continue\n",
    "                max_value = -np.inf\n",
    "                max_action = None\n",
    "                for action in self.actions[state]:\n",
    "                    new_value = sum(self.P[action][i, j] * (self.R[action][i, j] + gamma * V[j]) for j in range(self.size))\n",
    "                    if new_value > max_value:\n",
    "                        max_value = new_value\n",
    "                        max_action = action\n",
    "                new_policy[state] = max_action\n",
    "\n",
    "            # Stop criterion 1: no change in optimal policy\n",
    "            if new_policy == policy:\n",
    "                 break\n",
    "            # Compute approximate value of improved policy\n",
    "            Vnew = self._policy_value_function(new_policy, gamma, method, None, relaxations, V, 'array')\n",
    "            # Stop criterion 2: Change in V smaller than stop_tol\n",
    "            if np.max(np.abs(Vnew - V)) < stop_tol:\n",
    "                break\n",
    "            # Update for next iteration\n",
    "            V = Vnew\n",
    "            policy = new_policy\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f'Maximum number of iterations reached in policy_value_function. Final delta: {max_delta:5.3e}',\n",
    "                FiniteMDPWarning,\n",
    "                stacklevel=2,\n",
    "            )   \n",
    "        # Compute higher precision approximation for value function of final policy\n",
    "        V = self._policy_value_function(policy, gamma, method, stop_tol, max_iterations, V, 'dict')\n",
    "\n",
    "        return V, policy\n",
    "\n",
    "    def reset(self, initial_state=None):\n",
    "        if initial_state is None:\n",
    "            while True:\n",
    "                index = self.rng.integers(0, len(self.states))\n",
    "                if index not in self.terminal_indices:\n",
    "                    break\n",
    "            self.current_state_index = index\n",
    "        else:\n",
    "            self.current_state_index = self.states.index(initial_state)\n",
    "        self.terminated = False\n",
    "        return self.states[self.current_state_index]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_state_index is None:\n",
    "            raise RuntimeError('MDP not initialized, call reset() before calling step() for the first time')\n",
    "        if self.terminated:\n",
    "            raise RuntimeError('run terminated, call reset() to start a new run')\n",
    "\n",
    "        u = self.rng.random()\n",
    "        next_state_index = np.searchsorted(self.CP[action][self.current_state_index], u, side='right')\n",
    "        next_state =  self.states[next_state_index]\n",
    "        self.terminated = next_state_index in self.terminal_indices\n",
    "        reward = self.R[action][self.current_state_index, next_state_index]\n",
    "        self.current_state_index = next_state_index\n",
    "        state = self.states[self.current_state_index]\n",
    "        \n",
    "        return (state, reward, self.terminated)\n",
    "        \n",
    "    def sarsa(\n",
    "        self,\n",
    "        gamma=1.0,\n",
    "        alpha=0.1,\n",
    "        epsilon=0.1,\n",
    "        n_episodes=100_000,\n",
    "        max_steps_per_episode=10_000,\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tabular SARSA(0) consistent with (P, R).\n",
    "\n",
    "        - Rewards on transitions into terminal states are allowed.\n",
    "        - Terminal states have no actions (actions[state] is None).\n",
    "        - No bootstrapping from terminal states.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Initialize Q(s,a) only for non-terminal states\n",
    "        # ------------------------------------------------------------\n",
    "        Q = {}\n",
    "        for s in self.states:\n",
    "            if self.actions[s] is None:\n",
    "                continue\n",
    "            for a in self.actions[s]:\n",
    "                Q[(s, a)] = 0.0\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Epsilon-greedy policy (greedy over Q)\n",
    "        # ------------------------------------------------------------\n",
    "        def epsilon_greedy_action(state):\n",
    "            actions = self.actions[state]\n",
    "            if actions is None:\n",
    "                return None\n",
    "\n",
    "            if rng.random() < epsilon:\n",
    "                return rng.choice(actions)\n",
    "\n",
    "            q_vals = [Q[(state, a)] for a in actions]\n",
    "            return actions[int(np.argmax(q_vals))]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # SARSA learning loop\n",
    "        # ------------------------------------------------------------\n",
    "        for _ in range(n_episodes):\n",
    "\n",
    "            state = self.reset()\n",
    "            state_index = self.state_index[state]\n",
    "\n",
    "            if state_index in self.terminal_indices:\n",
    "                continue\n",
    "\n",
    "            action = epsilon_greedy_action(state)\n",
    "\n",
    "            for _ in range(max_steps_per_episode):\n",
    "\n",
    "                i = self.state_index[state]\n",
    "\n",
    "                next_state, _, terminated = self.step(action)\n",
    "                j = self.state_index[next_state]\n",
    "\n",
    "                # IMPORTANT: reward always comes from R\n",
    "                reward = self.R[action][i, j]\n",
    "\n",
    "                if terminated:\n",
    "                    # no bootstrap from terminal states\n",
    "                    Q[(state, action)] += alpha * (\n",
    "                        reward - Q[(state, action)]\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                next_action = epsilon_greedy_action(next_state)\n",
    "\n",
    "                Q[(state, action)] += alpha * (\n",
    "                    reward\n",
    "                    + gamma * Q[(next_state, next_action)]\n",
    "                    - Q[(state, action)]\n",
    "                )\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Extract greedy policy and value function\n",
    "        # ------------------------------------------------------------\n",
    "        policy = {}\n",
    "        V = {}\n",
    "\n",
    "        for s in self.states:\n",
    "            idx = self.state_index[s]\n",
    "\n",
    "            if idx in self.terminal_indices:\n",
    "                policy[s] = None\n",
    "                V[s] = 0.0\n",
    "                continue\n",
    "\n",
    "            actions = self.actions[s]\n",
    "            q_vals = [(Q[(s, a)], a) for a in actions]\n",
    "            best_q, best_a = max(q_vals, key=lambda x: x[0])\n",
    "\n",
    "            policy[s] = best_a\n",
    "            V[s] = best_q\n",
    "\n",
    "        return Q, policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c602c4-2504-4627-a6a4-b113b2a79779",
   "metadata": {},
   "source": [
    "# System description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfd0dc3-fe0e-40de-92eb-4eaf346d1594",
   "metadata": {},
   "source": [
    "We consider an wharehose that can hold up to $M$ items in inventory. The goal of the problem is to minimize costs in managing this inventory. This wharehouse holds a single kind of item.\n",
    "\n",
    "We assume that every day up to $K$ items can be ordered, and denote by $p_j$ the probability that $j$ items are ordered in each day ($j=0,1,\\ldots,K)$. Ordered items, if available, are removed from inventory by the beginning of the next day.\n",
    "\n",
    "Each day, the inventory manager can request up to $L$ items to replenish inventory, which will be delivered by the beginning of each day.\n",
    "\n",
    "The following are the operational costs in the system:\n",
    "\n",
    "- There is a holding cost $h_i$ for having $i$ items in inventory. We only assume that $h_i>h_j$ if $i>j$, but the way these costs grow can have any form (they can be linear, quadratic, or we can just assume some other kind of structure (convex, concave, etc.)\n",
    "\n",
    "- There is an ordering cost $b_i$ for requesting $i$ items to be added to inventory. This is also an increasing cost ($b_i>b_j$ if $i>j$), but we can have situations in which there is a \"jump\" in the costs (requesting $i+1$ items is **much** more expensive than requesting $i$ items.\n",
    "\n",
    "- There is a cost for not being able to deliver an item that is delivered. One simple hypothesis is that there is a unit cost $c$ for each lost item. However, we may want associate different costs for losing more than one order.\n",
    "\n",
    "Your task it to think how this could be formalized in an MDP model. To do this, you will have to come up with a very precise description of:\n",
    "\n",
    "- The state space.\n",
    "\n",
    "- The actions.\n",
    "\n",
    "- The transition probability matrices and reward matrices.\n",
    "\n",
    "Keep in mind that any model that has a chance to be realistic cannot be too small. So, for example the warehouse size $M$ should be something like 20 or so. So, our matrices will be $21\\times 21$, and we will have to use some programming even to set up these matrices!\n",
    "\n",
    "So, don't worry about it now. Think about what kind of data structures we would need to represent the parameters in the problem. If you don't know any programming, don't worry: think just in terms of mathematical modeling! We will figure out together how to come up with a mathematical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f98dfe0-1cd3-44a8-9dc6-004880488d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
