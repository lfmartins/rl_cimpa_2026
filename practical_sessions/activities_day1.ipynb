{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "974f4ff1-4247-4b0f-af7b-d959af4b4023",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a0420f-68b9-42ee-b163-fb3e14a8099f",
   "metadata": {},
   "source": [
    "The main goal of this notebook is to get used to the computational environment provided by [Colab](https://colab.research.google.com/). This environment provides free of charge access to computational resources suitable for learning and experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b8c37-1bba-4e75-9171-f16b86065031",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a547a-9494-4362-b843-22a9099e56a6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Before starting, we need to run the following two initialization cells. \n",
    "\n",
    "To run a cell, click on it and then either press Shift-Enter or click the \"play\" button on the toolbar at the top of the menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2daba05c-f054-4e1a-a62c-a1672b62607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab06a4-b92a-4826-a8d7-39a3e6b8a784",
   "metadata": {},
   "source": [
    "The cell above imports the module $\\mathtt{numpy}$, which contains highly efficient methods for numerical computation involving arrays.\n",
    "\n",
    "The next cell defines a class $\\mathtt{FiniteMDP}$, which is a simple implementation of a finite Markov Decision Process. It is not meant for \"production\" use, so there is not error checking or concerns with efficiency.\n",
    "\n",
    "There is a lot of code here, but don't be concerned with it.\n",
    "\n",
    "It is recommended that you collapse the **Initialization* section by clicking on the little triangle next to the section heading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd68144-527c-4b66-8738-3d948de361bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "class FiniteMDPWarning(UserWarning):\n",
    "    pass\n",
    "\n",
    "class FiniteMDP:\n",
    "    def __init__(self, states, actions, P, R, seed=None):\n",
    "        self.states = states.copy()\n",
    "        self.size = len(self.states)\n",
    "        self.state_index = {s: i for i, s in enumerate(states)}\n",
    "        self.actions = actions.copy()        \n",
    "        self.P = {\n",
    "            action:  np.array(M, dtype=np.float64, copy=True)\n",
    "            for action, M in P.items()\n",
    "        }\n",
    "\n",
    "        self.CP = {}\n",
    "        for action, P_a in self.P.items():        \n",
    "            self.CP[action] = np.cumsum(P_a, axis=1)\n",
    "            self.CP[action][:, -1] = 1.0\n",
    "        self.CP[action][:, -1] = 1.0\n",
    "\n",
    "        self.R = {\n",
    "            action:  np.array(M, dtype=np.float64, copy=True)\n",
    "            for action, M in R.items()\n",
    "        }\n",
    "        \n",
    "        self.P[None] = np.eye(self.size, dtype=np.float64)\n",
    "        self.R[None] = np.zeros(shape=(self.size, self.size), dtype=np.float64)\n",
    "        self.terminal_indices = {i for i, state in enumerate(states) if not actions[state]}\n",
    "\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        self.current_state_index = None\n",
    "        self.terminated = True        \n",
    "\n",
    "    def _policy_value_function_lu(self, indexed_policy, gamma):\n",
    "        PP = np.array([self.P[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        RR = np.array([self.R[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        b = np.sum(PP * RR, axis=1)\n",
    "        A = np.eye(self.size) - gamma * PP\n",
    "        for i in range(self.size):\n",
    "            if i in self.terminal_indices:\n",
    "                A[i][i] = 1.0\n",
    "                b[i] = 0\n",
    "        return np.linalg.solve(A, b)\n",
    "\n",
    "    def _policy_value_function_jacobi(self, indexed_policy, gamma, stop_tol, max_iterations, start=None):\n",
    "        if start is None:\n",
    "            V = np.zeros(self.size, dtype=np.float64)\n",
    "        else:\n",
    "            V = start.copy()\n",
    "        PP = np.array([self.P[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        RR = np.array([self.R[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        b = np.sum(PP * RR, axis=1)\n",
    "        for _ in range(max_iterations):\n",
    "            Vnew = b + gamma * PP @ V\n",
    "            sup_norm =  np.max(np.abs(V - Vnew))\n",
    "            if stop_tol is not None and sup_norm < stop_tol:\n",
    "                break\n",
    "            V = Vnew\n",
    "        else:\n",
    "            if stop_tol is not None:\n",
    "                warnings.warn(\n",
    "                    f'Maximum number of iterations reached in policy_value_function. Final delta: {sup_norm:5.3e}',\n",
    "                    FiniteMDPWarning,\n",
    "                    stacklevel=3,\n",
    "                )   \n",
    "        return V\n",
    "        \n",
    "    def _policy_value_function_gs(self, indexed_policy, gamma, stop_tol, max_iterations, start=None):\n",
    "        if start is None:\n",
    "            V = np.zeros(self.size, dtype=np.float64)\n",
    "        else:\n",
    "            V = start.copy()\n",
    "        PP = np.array([self.P[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        RR = np.array([self.R[indexed_policy[i]][i] for i in range(self.size)], dtype=np.float64)\n",
    "        b = np.sum(PP * RR, axis=1)\n",
    "        max_delta = -np.inf\n",
    "        for _ in range(max_iterations):\n",
    "            sup_norm = 0.0\n",
    "            for i in range(self.size):\n",
    "                new_value = b[i] + gamma * PP[i] @ V\n",
    "                sup_norm = max(sup_norm, abs(new_value - V[i]))\n",
    "                V[i] = new_value\n",
    "            if stop_tol is not None and sup_norm < stop_tol:\n",
    "                break\n",
    "        else:\n",
    "            if stop_tol is not None:\n",
    "                warnings.warn(\n",
    "                    f'Maximum number of iterations reached in policy_value_function. Final delta: {sup_norm:5.3e}',\n",
    "                    FiniteMDPWarning,\n",
    "                    stacklevel=3,\n",
    "                )   \n",
    "        return V\n",
    "        \n",
    "    def _policy_value_function(self, policy, gamma, method, stop_tol, max_iterations, start, return_type):\n",
    "        indexed_policy = self.size * [None]\n",
    "        for state, action in policy.items():\n",
    "            indexed_policy[self.state_index[state]] = action\n",
    "        match method:\n",
    "            case 'lu':\n",
    "                VV = self._policy_value_function_lu(indexed_policy, gamma)\n",
    "            case 'jacobi':\n",
    "                VV = self._policy_value_function_jacobi(indexed_policy, gamma, stop_tol, max_iterations, start)\n",
    "            case 'gs':\n",
    "                VV = self._policy_value_function_gs(indexed_policy, gamma, stop_tol, max_iterations, start)\n",
    "            case _:\n",
    "                raise ValueError(\"method should be 'lu', 'jacobi' or 'gs'\")\n",
    "\n",
    "        match return_type:\n",
    "            case 'dict':\n",
    "                return {state: VV[self.state_index[state]] for state in self.states}\n",
    "            case 'array':\n",
    "                return VV\n",
    "            case _:\n",
    "                raise ValueError(\"return_type must be 'dict' or 'array'\")\n",
    "\n",
    "    def policy_value_function(self, policy, gamma=1, method='lu', stop_tol=1E-8, max_iterations=100, start=None, return_type='dict'):\n",
    "        if start is not None:\n",
    "            start = np.array([start[s] for s in self.states])\n",
    "        return self._policy_value_function(policy, gamma, method, stop_tol, max_iterations, start, return_type)\n",
    "\n",
    "    def value_iteration(self, gamma=1, stop_tol=1E-8, max_iterations=100, start=None):\n",
    "        if start is None:\n",
    "            V = np.zeros(self.size, dtype=np.float64)\n",
    "        else:\n",
    "            V = np.array([start[s] for s in states], dtype=np.float64)\n",
    "            for i in range(self.size):\n",
    "                if i in self.terminal_indices:\n",
    "                    V[i] = 0.0\n",
    "        for _ in range(max_iterations):\n",
    "            max_delta = 0.0\n",
    "            for i, state in enumerate(self.states):\n",
    "                if i in self.terminal_indices:\n",
    "                    continue\n",
    "                new_value = -np.inf\n",
    "                for action in self.actions[state]:\n",
    "                    new_value = max(new_value, \n",
    "                                    sum(self.P[action][i, j] * (self.R[action][i, j] + gamma * V[j]) for j in range(self.size)))\n",
    "                if stop_tol is not None:\n",
    "                    max_delta = max(max_delta, abs(V[i] - new_value))\n",
    "                V[i] = new_value\n",
    "            if stop_tol is not None and max_delta < stop_tol:\n",
    "                break\n",
    "        else:\n",
    "                warnings.warn(\n",
    "                    f'Maximum number of iterations reached in policy_value_function. Final delta: {max_delta:5.3e}',\n",
    "                    FiniteMDPWarning,\n",
    "                    stacklevel=2,\n",
    "                )   \n",
    "        # Compute optimal policy\n",
    "        policy = {}\n",
    "        for i,state in enumerate(self.states):\n",
    "            if i in self.terminal_indices:\n",
    "                policy[state] = None\n",
    "                continue\n",
    "            max_value = -np.inf\n",
    "            max_action = None\n",
    "            for action in self.actions[state]:\n",
    "                new_value = sum(self.P[action][i, j] * (self.R[action][i, j] + gamma * V[j]) for j in range(self.size))\n",
    "                if new_value > max_value:\n",
    "                    max_value = new_value\n",
    "                    max_action = action\n",
    "            policy[state] = max_action\n",
    "\n",
    "        return {state: V[i] for i, state in enumerate(self.states)}, policy\n",
    "\n",
    "    def policy_iteration(self, gamma=1, method='lu', stop_tol=1E-8, max_iterations=100, relaxations=20, start=None):\n",
    "        if start is None:\n",
    "            V = np.zeros(self.size, dtype=np.float64)\n",
    "        else:\n",
    "            V = np.array([start[s] for s in states], dtype=np.float64)\n",
    "            for i in range(self.size):\n",
    "                if i in self.terminal_indices:\n",
    "                    V[i] = 0.0\n",
    "\n",
    "        # Initialize random policy\n",
    "        policy = {}\n",
    "        for i, state in enumerate(self.states):\n",
    "            if i in self.terminal_indices:\n",
    "                policy[state] = None\n",
    "                continue\n",
    "            policy[state] = self.rng.choice(self.actions[state])\n",
    "\n",
    "        # Compute approximate value of current policy\n",
    "        V = self._policy_value_function(policy, gamma, method, None, relaxations, None, 'array')\n",
    "\n",
    "        for _ in range(max_iterations):\n",
    "            # Compute improved policy\n",
    "            new_policy = {}\n",
    "            for i, state in enumerate(self.states):\n",
    "                if i in self.terminal_indices:\n",
    "                    new_policy[state] = None\n",
    "                    continue\n",
    "                max_value = -np.inf\n",
    "                max_action = None\n",
    "                for action in self.actions[state]:\n",
    "                    new_value = sum(self.P[action][i, j] * (self.R[action][i, j] + gamma * V[j]) for j in range(self.size))\n",
    "                    if new_value > max_value:\n",
    "                        max_value = new_value\n",
    "                        max_action = action\n",
    "                new_policy[state] = max_action\n",
    "\n",
    "            # Stop criterion 1: no change in optimal policy\n",
    "            if new_policy == policy:\n",
    "                 break\n",
    "            # Compute approximate value of improved policy\n",
    "            Vnew = self._policy_value_function(new_policy, gamma, method, None, relaxations, V, 'array')\n",
    "            # Stop criterion 2: Change in V smaller than stop_tol\n",
    "            if np.max(np.abs(Vnew - V)) < stop_tol:\n",
    "                break\n",
    "            # Update for next iteration\n",
    "            V = Vnew\n",
    "            policy = new_policy\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                f'Maximum number of iterations reached in policy_value_function. Final delta: {max_delta:5.3e}',\n",
    "                FiniteMDPWarning,\n",
    "                stacklevel=2,\n",
    "            )   \n",
    "        # Compute higher precision approximation for value function of final policy\n",
    "        V = self._policy_value_function(policy, gamma, method, stop_tol, max_iterations, V, 'dict')\n",
    "\n",
    "        return V, policy\n",
    "\n",
    "    def reset(self, initial_state=None):\n",
    "        if initial_state is None:\n",
    "            while True:\n",
    "                index = self.rng.integers(0, len(self.states))\n",
    "                if index not in self.terminal_indices:\n",
    "                    break\n",
    "            self.current_state_index = index\n",
    "        else:\n",
    "            self.current_state_index = self.states.index(initial_state)\n",
    "        self.terminated = False\n",
    "        return self.states[self.current_state_index]\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_state_index is None:\n",
    "            raise RuntimeError('MDP not initialized, call reset() before calling step() for the first time')\n",
    "        if self.terminated:\n",
    "            raise RuntimeError('run terminated, call reset() to start a new run')\n",
    "\n",
    "        u = self.rng.random()\n",
    "        next_state_index = np.searchsorted(self.CP[action][self.current_state_index], u, side='right')\n",
    "        next_state =  self.states[next_state_index]\n",
    "        self.terminated = next_state_index in self.terminal_indices\n",
    "        reward = self.R[action][self.current_state_index, next_state_index]\n",
    "        self.current_state_index = next_state_index\n",
    "        state = self.states[self.current_state_index]\n",
    "        \n",
    "        return (state, reward, self.terminated)\n",
    "        \n",
    "    def sarsa(\n",
    "        self,\n",
    "        gamma=1.0,\n",
    "        alpha=0.1,\n",
    "        epsilon=0.1,\n",
    "        n_episodes=100_000,\n",
    "        max_steps_per_episode=10_000,\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Tabular SARSA(0) consistent with (P, R).\n",
    "\n",
    "        - Rewards on transitions into terminal states are allowed.\n",
    "        - Terminal states have no actions (actions[state] is None).\n",
    "        - No bootstrapping from terminal states.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Initialize Q(s,a) only for non-terminal states\n",
    "        # ------------------------------------------------------------\n",
    "        Q = {}\n",
    "        for s in self.states:\n",
    "            if self.actions[s] is None:\n",
    "                continue\n",
    "            for a in self.actions[s]:\n",
    "                Q[(s, a)] = 0.0\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Epsilon-greedy policy (greedy over Q)\n",
    "        # ------------------------------------------------------------\n",
    "        def epsilon_greedy_action(state):\n",
    "            actions = self.actions[state]\n",
    "            if actions is None:\n",
    "                return None\n",
    "\n",
    "            if rng.random() < epsilon:\n",
    "                return rng.choice(actions)\n",
    "\n",
    "            q_vals = [Q[(state, a)] for a in actions]\n",
    "            return actions[int(np.argmax(q_vals))]\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # SARSA learning loop\n",
    "        # ------------------------------------------------------------\n",
    "        for _ in range(n_episodes):\n",
    "\n",
    "            state = self.reset()\n",
    "            state_index = self.state_index[state]\n",
    "\n",
    "            if state_index in self.terminal_indices:\n",
    "                continue\n",
    "\n",
    "            action = epsilon_greedy_action(state)\n",
    "\n",
    "            for _ in range(max_steps_per_episode):\n",
    "\n",
    "                i = self.state_index[state]\n",
    "\n",
    "                next_state, _, terminated = self.step(action)\n",
    "                j = self.state_index[next_state]\n",
    "\n",
    "                # IMPORTANT: reward always comes from R\n",
    "                reward = self.R[action][i, j]\n",
    "\n",
    "                if terminated:\n",
    "                    # no bootstrap from terminal states\n",
    "                    Q[(state, action)] += alpha * (\n",
    "                        reward - Q[(state, action)]\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "                next_action = epsilon_greedy_action(next_state)\n",
    "\n",
    "                Q[(state, action)] += alpha * (\n",
    "                    reward\n",
    "                    + gamma * Q[(next_state, next_action)]\n",
    "                    - Q[(state, action)]\n",
    "                )\n",
    "\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "        # ------------------------------------------------------------\n",
    "        # Extract greedy policy and value function\n",
    "        # ------------------------------------------------------------\n",
    "        policy = {}\n",
    "        V = {}\n",
    "\n",
    "        for s in self.states:\n",
    "            idx = self.state_index[s]\n",
    "\n",
    "            if idx in self.terminal_indices:\n",
    "                policy[s] = None\n",
    "                V[s] = 0.0\n",
    "                continue\n",
    "\n",
    "            actions = self.actions[s]\n",
    "            q_vals = [(Q[(s, a)], a) for a in actions]\n",
    "            best_q, best_a = max(q_vals, key=lambda x: x[0])\n",
    "\n",
    "            policy[s] = best_a\n",
    "            V[s] = best_q\n",
    "\n",
    "        return Q, policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2da8620-b00d-4b03-853f-72914043cc20",
   "metadata": {},
   "source": [
    "# Example - Recycling Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d21c56-3306-4ddc-8f64-fa5b09185e3e",
   "metadata": {},
   "source": [
    "In this first example, we use the Recycling Robot example from the book [*Reinforcement Learning - An Introduction* by Sutton and Barto](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf)  \n",
    "\n",
    "The transition probability matrices for each action are:\n",
    "\n",
    "$$\n",
    "P^{\\mathtt{search}}=\n",
    "\\begin{bmatrix}\n",
    "\\alpha & 1-\\alpha\\\\\n",
    "1-\\beta & \\beta\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "P^{\\texttt{wait}}=\n",
    "\\begin{bmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\qquad\n",
    "P^\\mathtt{recharge}=\n",
    "\\begin{bmatrix}\n",
    "- & -\\\\\n",
    "1 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And the reward matrices are:\n",
    "\n",
    "$$\n",
    "R^{\\mathtt{search}}=\n",
    "\\begin{bmatrix}\n",
    "r_{\\mathtt{search}} & r_{\\mathtt{search}}\\\\\n",
    "r_{\\mathtt{empty}} & r_{\\mathtt{search}}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "R^{\\mathtt{wait}}=\n",
    "\\begin{bmatrix}\n",
    "r_{\\mathtt{wait}} & - \\\\\n",
    "- & r_{\\mathtt{wait}}\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "R^{\\mathtt{recharge}}=\n",
    "\\begin{bmatrix}\n",
    "- & -\\\\\n",
    "0 & -\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The dashes $-$ represent entries that are not meaningful, since they represent actions that are not allowed in a given state. They can have any value and, in practice, are symply set to zero. \n",
    "\n",
    "To represent the model computationally, we first define the model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd73d8b5-7df6-4802-a71a-4899105a8be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters:\n",
      "alpha=0.8, beta=0.3, rsearch=15, rwait=10, rempty=-3.0\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.8\n",
    "beta = 0.3\n",
    "rsearch = 15\n",
    "rwait = 10\n",
    "rempty = -3.0\n",
    "print(f'Model parameters:\\n{alpha=}, {beta=}, {rsearch=}, {rwait=}, {rempty=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15660b93-016b-4e42-bf6a-465c50e409ec",
   "metadata": {},
   "source": [
    "Let's now define the states and admissible actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d57e9a6d-53b0-43b9-a953-3e884e22c703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Admissible actions for state 'high': ['search', 'wait']\n",
      "Admissible actions for state 'low': ['search', 'wait', 'recharge']\n"
     ]
    }
   ],
   "source": [
    "states = ['high', 'low']\n",
    "actions = {\n",
    "    'high': ['search', 'wait'],\n",
    "    'low': ['search', 'wait', 'recharge']\n",
    "}\n",
    "print(f\"Admissible actions for state 'high': {actions['high']}\\n\"\n",
    "      f\"Admissible actions for state 'low': {actions['low']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc6b218-ee2a-4a2e-bd49-c9766892e832",
   "metadata": {},
   "source": [
    "Next, we define the transition probability and reward matrices for each action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f1424a-c295-4a0b-830e-626c12a2131c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probability matrix for action search:\n",
      "[[0.8 0.2]\n",
      " [0.7 0.3]]\n",
      "Transition probability matrix for action wait:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "Transition probability matrix for action recharge:\n",
      "[[0. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "P = {}\n",
    "P['search'] = np.array([[alpha, 1 - alpha], [1 - beta, beta]], dtype=np.float64)\n",
    "P['wait'] = np.array([[1, 0], [0, 1]], dtype=np.float64)\n",
    "P['recharge'] = np.array([[0, 0], [1, 0]], dtype=np.float64)\n",
    "\n",
    "for key in P.keys():\n",
    "    print(f'Transition probability matrix for action {key}:')\n",
    "    print(P[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd05bae7-a514-4ba6-9bd1-2a5112dd19ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward matrix for action search:\n",
      "[[15. 15.]\n",
      " [-3. 15.]]\n",
      "Reward matrix for action wait:\n",
      "[[10.  0.]\n",
      " [ 0. 10.]]\n",
      "Reward matrix for action recharge:\n",
      "[[0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "R = {}\n",
    "R['search'] = np.array([[rsearch, rsearch], [rempty, rsearch]], dtype=np.float64)\n",
    "R['wait'] = np.array([[rwait, 0], [0, rwait]], dtype=np.float64)\n",
    "R['recharge'] = np.array([[0, 0], [0, 0]], dtype=np.float64)\n",
    "\n",
    "for key in R.keys():\n",
    "    print(f'Reward matrix for action {key}:')\n",
    "    print(R[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0a7408-8228-4eae-858b-e10a2e1fd070",
   "metadata": {},
   "source": [
    "We are now ready to define the `FiniteMDP` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4817191d-575a-47f0-9b19-184cfa11af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_model = FiniteMDP(states, actions, P, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79476cc-3335-44b2-a4f9-10be5d0db9ca",
   "metadata": {},
   "source": [
    "## Simulating the chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e5b09f-ee51-48a3-9fca-30de4158c7f0",
   "metadata": {},
   "source": [
    "Let's now simulate the chain. We start by defining a random number generator, to generate random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be9812ac-60f5-4bd7-b05f-ff89d0d80748",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(77)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161cb084-23e0-47b1-ba82-239c098131c7",
   "metadata": {},
   "source": [
    "The next cell simulates $\\mathtt{n\\_steps}$ steps of the chain (recall that this is a continuing task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27cb7525-fdb9-4bd5-bb02-4f920f8e5026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial state: high\n",
      "Step  1: action=   search, state= high, reward= 15.00, return=  15.000, terminated=False\n",
      "Step  2: action=     wait, state= high, reward= 10.00, return=  24.000, terminated=False\n",
      "Step  3: action=     wait, state= high, reward= 10.00, return=  32.100, terminated=False\n",
      "Step  4: action=     wait, state= high, reward= 10.00, return=  39.390, terminated=False\n",
      "Step  5: action=     wait, state= high, reward= 10.00, return=  45.951, terminated=False\n",
      "Step  6: action=   search, state= high, reward= 15.00, return=  54.808, terminated=False\n",
      "Step  7: action=     wait, state= high, reward= 10.00, return=  60.123, terminated=False\n",
      "Step  8: action=   search, state= high, reward= 15.00, return=  67.297, terminated=False\n",
      "Step  9: action=     wait, state= high, reward= 10.00, return=  71.602, terminated=False\n",
      "Step 10: action=   search, state= high, reward= 15.00, return=  77.413, terminated=False\n",
      "Step 11: action=   search, state= high, reward= 15.00, return=  82.643, terminated=False\n",
      "Step 12: action=   search, state=  low, reward= 15.00, return=  87.351, terminated=False\n",
      "Step 13: action=   search, state= high, reward= -3.00, return=  86.503, terminated=False\n",
      "Step 14: action=     wait, state= high, reward= 10.00, return=  89.045, terminated=False\n",
      "Step 15: action=   search, state= high, reward= 15.00, return=  92.477, terminated=False\n",
      "Step 16: action=   search, state= high, reward= 15.00, return=  95.565, terminated=False\n",
      "Step 17: action=     wait, state= high, reward= 10.00, return=  97.418, terminated=False\n",
      "Step 18: action=   search, state= high, reward= 15.00, return=  99.920, terminated=False\n",
      "Step 19: action=   search, state= high, reward= 15.00, return= 102.171, terminated=False\n",
      "Step 20: action=     wait, state= high, reward= 10.00, return= 103.522, terminated=False\n"
     ]
    }
   ],
   "source": [
    "current_state = rr_model.reset()\n",
    "n_steps = 20\n",
    "gamma = 0.9\n",
    "discount = 1.0\n",
    "total_return = 0.0\n",
    "print(f'Initial state: {current_state}')\n",
    "for i in range(n_steps):\n",
    "    action = rng.choice(actions[current_state])\n",
    "    next_state, reward, terminated = rr_model.step(action)\n",
    "    total_return += discount * reward\n",
    "    discount *= gamma\n",
    "    current_state = next_state\n",
    "    print(f'Step {i + 1:2d}: action={action:>9}, state={next_state:>5}, reward={reward:6.2f}, return={total_return:8.3f}, terminated={terminated}')\n",
    "    if terminated:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439bab8-ebde-4575-a014-350d1cf0f72e",
   "metadata": {},
   "source": [
    "## Computation of State Value Function for a Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79468323-8f95-4fe8-86e3-3d97e0872274",
   "metadata": {},
   "source": [
    "We now turn to the problem of computing the state value function of a policy. In this case, the number of deterministic policies is small, so we just enumerate all policies in a list and compute the value function using different methods as a check.\n",
    "\n",
    "Computing the value function associated to a policy is called *evaluating* the policy and is a crucial step in any RL algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9289c430-f15b-4381-a93e-5714922f96eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rr_policies = [\n",
    "    {'high': 'search', 'low': 'search'},\n",
    "    {'high': 'search', 'low': 'wait'},\n",
    "    {'high': 'search', 'low': 'recharge'},\n",
    "    {'high': 'wait', 'low': 'search'},\n",
    "    {'high': 'wait', 'low': 'wait'},\n",
    "    {'high': 'wait', 'low': 'recharge'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438d10c-109e-4740-bf27-df0c950756e4",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4162e9bf-d519-42aa-99a7-27e87bd58f41",
   "metadata": {},
   "source": [
    "\n",
    "In the next code cell, for each policy, we compute the returns for $\\mathtt{n\\_runs}$ of the Markov chain. The runs are truncated at $\\mathtt{n\\_steps}$ (we need to truncate because this is a continuing task).\n",
    "\n",
    "(A more efficient version of this code would use a \"first visit\" strategy in each run.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ca871b0-0a2d-43b9-bc69-9af4982b1e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: pi(high)=search, pi(low)=search. Value function: V(high)=124.00947, V(low)=110.88677\n",
      "Policy: pi(high)=search, pi(low)=wait. Value function: V(high)=116.71515, V(low)=100.00000\n",
      "Policy: pi(high)=search, pi(low)=recharge. Value function: V(high)=126.62145, V(low)=115.14574\n",
      "Policy: pi(high)=wait, pi(low)=search. Value function: V(high)=100.00000, V(low)=89.69136\n",
      "Policy: pi(high)=wait, pi(low)=wait. Value function: V(high)=100.00000, V(low)=100.00000\n",
      "Policy: pi(high)=wait, pi(low)=recharge. Value function: V(high)=100.00000, V(low)=90.00000\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "n_steps = 1000\n",
    "n_trials = 200\n",
    "\n",
    "for policy in rr_policies:\n",
    "    V = {'high': 0.0, 'low': 0.0}\n",
    "    for state in ['high', 'low']:\n",
    "        for n in range(n_trials):\n",
    "            discount = 1.0\n",
    "            current_state = rr_model.reset(initial_state=state)\n",
    "            total_return = 0.0\n",
    "            for i in range(n_steps):\n",
    "                action = policy[current_state]\n",
    "                next_state, reward, terminated = rr_model.step(action)\n",
    "                total_return += discount * reward\n",
    "                discount *= gamma\n",
    "                current_state = next_state\n",
    "                if terminated:\n",
    "                    break\n",
    "            V[state] += 1 / (n + 1) * (total_return - V[state])\n",
    "    print(f\"Policy: pi(high)={policy['high']}, pi(low)={policy['low']}. \"\n",
    "          f\"Value function: V(high)={V['high']:8.5f}, V(low)={V['low']:8.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b535b7-52b9-447b-bfed-b497f3e1a9fe",
   "metadata": {},
   "source": [
    "### LU Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75af64b2-340b-4b84-af0a-408d0b0a33eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: pi(high)=search, pi(low)=search. Value function: V(high)=125.07692, V(low)=111.23077\n",
      "Policy: pi(high)=search, pi(low)=wait. Value function: V(high)=117.85714, V(low)=100.00000\n",
      "Policy: pi(high)=search, pi(low)=recharge. Value function: V(high)=127.11864, V(low)=114.40678\n",
      "Policy: pi(high)=wait, pi(low)=search. Value function: V(high)=100.00000, V(low)=89.58904\n",
      "Policy: pi(high)=wait, pi(low)=wait. Value function: V(high)=100.00000, V(low)=100.00000\n",
      "Policy: pi(high)=wait, pi(low)=recharge. Value function: V(high)=100.00000, V(low)=90.00000\n"
     ]
    }
   ],
   "source": [
    "for rr_policy in rr_policies:\n",
    "    V = rr_model.policy_value_function(rr_policy, gamma=0.9)\n",
    "    print(f\"Policy: pi(high)={rr_policy['high']}, pi(low)={rr_policy['low']}. \"\n",
    "          f\"Value function: V(high)={V['high']:8.5f}, V(low)={V['low']:8.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0878a3-b274-4afd-bfef-31cfcd40f184",
   "metadata": {},
   "source": [
    "### Jacobi iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30053075-4dd3-4029-8188-465921890fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: pi(high)=search, pi(low)=search. Value function: V(high)=125.07692, V(low)=111.23077\n",
      "Policy: pi(high)=search, pi(low)=wait. Value function: V(high)=117.85714, V(low)=100.00000\n",
      "Policy: pi(high)=search, pi(low)=recharge. Value function: V(high)=127.11864, V(low)=114.40678\n",
      "Policy: pi(high)=wait, pi(low)=search. Value function: V(high)=100.00000, V(low)=89.58904\n",
      "Policy: pi(high)=wait, pi(low)=wait. Value function: V(high)=100.00000, V(low)=100.00000\n",
      "Policy: pi(high)=wait, pi(low)=recharge. Value function: V(high)=100.00000, V(low)=90.00000\n"
     ]
    }
   ],
   "source": [
    "for rr_policy in rr_policies:\n",
    "    V = rr_model.policy_value_function(rr_policy, gamma=0.9, method='jacobi', max_iterations=200)\n",
    "    print(f\"Policy: pi(high)={rr_policy['high']}, pi(low)={rr_policy['low']}. \"\n",
    "          f\"Value function: V(high)={V['high']:8.5f}, V(low)={V['low']:8.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f88fb-f7f8-49aa-80c8-f6cc7102224e",
   "metadata": {},
   "source": [
    "### Gauss-Seidel iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdcd8256-6f8e-404a-94a3-af69ba5ca6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: pi(high)=search, pi(low)=search. Value function: V(high)=125.07692, V(low)=111.23077\n",
      "Policy: pi(high)=search, pi(low)=wait. Value function: V(high)=117.85714, V(low)=100.00000\n",
      "Policy: pi(high)=search, pi(low)=recharge. Value function: V(high)=127.11864, V(low)=114.40678\n",
      "Policy: pi(high)=wait, pi(low)=search. Value function: V(high)=100.00000, V(low)=89.58904\n",
      "Policy: pi(high)=wait, pi(low)=wait. Value function: V(high)=100.00000, V(low)=100.00000\n",
      "Policy: pi(high)=wait, pi(low)=recharge. Value function: V(high)=100.00000, V(low)=90.00000\n"
     ]
    }
   ],
   "source": [
    "for rr_policy in rr_policies:\n",
    "    V = rr_model.policy_value_function(rr_policy, gamma=0.9, method='gs', max_iterations=200)\n",
    "    print(f\"Policy: pi(high)={rr_policy['high']}, pi(low)={rr_policy['low']}. \"\n",
    "          f\"Value function: V(high)={V['high']:8.5f}, V(low)={V['low']:8.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3749707-03a4-447c-9715-d3cca3a726e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18bcea2-5781-43e9-be5b-148691c586f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ec450-ff55-4b4a-b64f-015b3c9c2a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
